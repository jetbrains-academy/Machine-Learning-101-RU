Наивный Байесов классификатор работает в предположении о независимости признаков объекта. В нашем случае это означает,
что вероятность встретить некоторое слово в сообщении не зависит от наличия других слов в этом сообщении.
Т.к. мы векторизовали сообщения с учётом частот встречаемости слов, мы будем использовать мультиномиальную модель
классификатора и оценки будут несколько отличаться от тех, которые использовались на лекции.

Оценку для параметров для каждого из признаков можно записать следующим образом:
$$\theta_{yj} = \frac{\sum\limits_{i=1}^l [y_i = y] x_{ij}}{\sum\limits_{j \in V}\sum\limits_{i=1}^l [y_i = y] x_{ij}}$$

$x_{ij}$ -- значение $j$-го признака объекта $i$

$V$ -- словарь входных данных.

Другими словами, числитель описывает сколько раз слово встречается в сообщениях класса $y$ (включая повторы),
а знаменатель – это суммарное количество слов во всех документах этого класса.

Реализуйте метод `fit`, который по переданной выборке вычисляет следующие параметры, которые понадобятся на этапе классификации:
  - оценка априорной вероятности классов $\hat{P_y}$
  - относительные частоты слов для каждого класса
  - суммарное количество слов для сообщений каждого класса
  - размер словаря выборки
