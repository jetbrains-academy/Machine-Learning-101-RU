Теперь необходимо преобразовать вычисляемую вероятность в нечто, что можно вычислить использую частоту встречаемости слов. Для этого можно использовать некоторые базовые свойства вероятностей и [теорему Байеса](https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%91%D0%B0%D0%B9%D0%B5%D1%81%D0%B0).

Теорема полезна при работе с условными вероятностями (например, в нашем случае), так как позволяет их обратить:

$$P(A|B) = \frac{P(B|A) \times P(A)}{P(B)}$$

В нашем случае имеется $P(spam | sentence)$, так что с помощью теоремы мы можем получить:

$$P(spam|sentence) = \frac{P(sentence|spam) \times P(spam)}{P(sentence)}$$

Для нашего классификатора мы лишь пытаемся определить наиболее вероятный тэг, так что мы можем пренебречь знаменателем, одинаковым для обоих тэгов, и сравнить числители:

$$P(sentence|spam) \times P(spam)$$

и

$$P(sentence|ham) \times P(ham)$$

### Наивный

Наивный Байесовский классификатор предполагает, что наличие какого-либо из признаков не связано с наличием остальных, в связи с чем его и называют наивным (**naive**).

В нашем случае это означает, что вероятность встретить некоторое слово в сообщении не зависит от наличия других слов в этом сообщении.

$$P(\text{Who let the dogs out}) = P(\text{Who}) \times P(\text{let}) \times P(\text{the}) \times P(\text{dogs}) \times P(\text{out})$$

В нашей обучающей выборке вычисление вероятности сводится к подсчету. 


### Задание

Реализуйте метод `fit`, который по переданной выборке вычисляет и сохраняет следующие параметры, которые понадобятся на этапе классификации:

  -  `classes_prior` -- оценка априорной вероятности классов в виде numpy вектора длины 2 (количество классов)

  $$P (\text{spam}) = \frac{\text{Num documents that have been classified as spam}}{\text{Num documents}}$$

  - `likelihood` -- относительные частоты слов для каждого класса в виде вектора numpy размерности (`2 x размер словаря`)

  - `classes_words_count` -- суммарное количество слов для сообщений каждого класса в виде вектора длины 2
