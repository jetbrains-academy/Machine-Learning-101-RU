Линейный классификатор принимает решения по классификации основываясь на линейной предсказывающей функции, совмещающей набор весовых коэффициентов с вектором признаков.

Если входной вектор признаков для классификатора - вектор $\vec {x}$, тогда результатом будет

$$y=f(\vec {w} \cdot \vec {x}) = f ( \sum_{j} w_{j} x_{j})$$

где $\vec {w}$ - вектор весов, а $f$ - функция, преобразующая произведение двух векторов в желаемый результат. Вектор весов $\vec {w}$ тренируется на примере обучающих выборок.
Зачастую, f - пороговая функция, которая соотносит $ \vec {w} \cdot \vec {x}$ выше некоторого порога первому классу, а остальные значения - второму.

Для задачи классификации на два класса можно представить линейный классификатор как разбиение входных данных с высокой размерностью с помощью гиперплоскости: все объекты с одной стороны гиперплоскости классифицируются как "да", остальные же классифицируются как "нет".

Потерями (**loss**) называют ошибку в предсказанных значениях $\vec{w}$. Цель - минимизировать ошибку и получить наиболее точный результат.

Существует несколько способов вычислить потери. Мы будем использовать функции `log_loss` и `sigmoid_loss`.

Логарифмическая функция потерь:

$$L(M) = \log_2(1 + e^{-M})$$

Сигмоидальная функция потерь (она же сигмоида):

$$L(M) = 2(1 + e^{M})^{-1}$$

### Задание

Реализуйте логарифмическую (`log_loss`) и сигмоидную (`sigmoid_loss`) функции потерь. Функция потерь должна принимать на
вход вектор и возвращать пару из вектора значений функции потерь и вектора её производных. Например, если бы
мы решили использовать степенную функцию потерь:

    def power_loss(M, n=5):
        return M ** n, n * (M ** (n - 1))
