Стохастический градиентный спуск (**SGD**) вычисляет градиент для каждого обновления используя лишь один обучающий объект $x_i$ (выбранный случайно).
Идея подобных вычислений в том, что градиент вычисленный таким образом является стохастическим приближением градиента вычисленного на всей обучающей выборке.
Каждое обновление таким образом можно вычислить гораздо быстрее, чем при групповом градиентном спуске, а по совокупности многих обновлений алгоритм будет двигаться в основном в том же направлении.

В стохастическом градиентном спуске на основе небольшого количества образцов (**Stochastic mini-batch gradient descent**) вычисляется градиент для каждой небольшой группы объектов из тренировочных данных.

Для начала обучающая выборка делится на небольшие группы (к примеру, по `k` элементов). Обновление происходит для каждой подобной группы. В зависимости от задачи, `k` обычно принимает значения от 30 до 500.
### Задание

Реализуйте метод стохастического градиентного спуска для обучения линейного классификатора.
﻿

Как и в случае с `GradientDescent` метод `fit` должен
возвращать значения $Q$ на каждой итерации.
Чтобы оценить $Q$ на каждой итерации, воспользуйтесь формулой
$$Q = (1 − \eta)Q + \eta L_i$$

Значение $\eta \in [0, 1]$,
используемое для вычисления оценки $Q$, можно выбрать любое, например,
`1 / len(X)`. Так как величина $Q$ не стабильна, использовать
её для определения сходимости не следует.

Вместо этого предлагается использовать "стратегию оптимиста": сделать ровно `n_iter`
итераций и надеяться, что за это время стохастический градиентный спуск
сойдётся.

Параметр `k` определяет размер случайной подвыборки из
`X`, используемой для вычисления градиента.