Одна из проблем с наивным подходом заключается в следующем:

Если какое-то слово не встречалось в тренировочной выборке класса Spam, то его вероятность $P(word | Spam) = 0$.

Проблема с основанной на частоте встречаемости вероятностью равной 0 в том, что она скроет информацию обо всех остальных вероятностях.

Одним из возможных решений является [аддитивное сглаживание](https://en.wikipedia.org/wiki/Laplace_smoothing), или же **сглаживание Лапласа** - техника, позволяющая сгладить категорические данные. Небольшая корректирующая выборка встраивается в каждую вычисляемую вероятность.

Добавим к каждому количеству слов 1, чтобы оно никогда не равнялось 0. В противовес этому добавим количество возможных слов к знаменателю, чтобы результат деления никогда не превышал единицы.

### Задание

Обновите свою имплементацию метода `fit` так, чтобы использовать Laplace Smoothing.