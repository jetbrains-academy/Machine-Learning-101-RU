####Information Gain
Количество получаемой информации (**Information Gain**) основано на уменьшении энтропии после разбиения выборки по тому или иному признаку. Построение решающих деревьев заключается в нахождении признака, дающего наибольшее количество информации (иными словами, гомогенных ветвей).

$$IGain = H(parent) - H(children) $$

Мы вычитаем энтропию `Y` при условии `X` из энтропии `Y` для вычисления уменьшения неопределенности
`Y`, при условии наличия дополнительного знания `X` про `Y`.


### Задание

Реализуйте метод `information_gain`, который принимает выборку, разделяет ее на 2 независимые подвыборки и
подсчитывает information gain.
