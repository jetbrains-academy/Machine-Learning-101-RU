Так как мы имеем случайный набор весов, необходимо изменить его так, чтобы входные данные из датасета совпадали с соответствующими выходными после преобразования.
Это осуществляется с помощью процедуры обратной связи.

Обратная связь работает за счет функции потерь, показывающей, насколько далеко сеть была от целевого результата.

### Потери

Один из способов вычисления функции потерь - использование суммарной квадратичной ошибки (sum square loss function:

$$Loss(y, \hat{y}) = \sum\limits_{i=1}^{n} (y_i - \hat{y}_i) ^ 2$$

где $\hat{y}$ - предсказанные выходные данные

$y$ - реальные выходные данные

Сумма квадратов ошибок (**sum-of-squares error**, **SSE**) - сумма разностей между каждым спрогнозированным значением и реальными данными. Разность возведена в квадрат, чтобы оперировать ее абсолютным значением.

Задача обучения - найти наилучший набор весов и смещений, минимизирующий функцию потерь.

Для того чтобы знать, насколько необходимо изменить веса и смещения, необходимо узнать зависимость производной функции потерь от них.

В одном из предыдущих уроков (**Градиентный спуск**, **Gradient Descent**) описывалось, что производная это наклон функции. Если известна производная, можно обновлять веса и смещения, увеличивая или уменьшая их с ее помощью.

Тем не менее, невозможно просто вычислить данную зависимость, так как уравнение функции потерь не содержит в себе ни весов, ни смещений. Для подобных вычислений необходимо определить некое связующее правило.

$$\frac {\partial Loss(y, \hat{y})}{\partial W} =  \frac { \partial Loss(y, \hat{y} ) } {\partial \hat{y}}
\frac { \partial \hat{y} } {\partial z} \frac { \partial z } {\partial W} $$

$$= 2 (y - \hat{y} ) * z (1- z) * x$$

где $z = Wx + b$

Вот каким образом можно вычислить прирост весов:

- Найти предел ошибки выходного слоя как разницу предсказанных выходных данных и реальных

- Применить производную сигмоидальной функции активации к ошибке выходных данных. Назовем результат дельтой выходной суммы (**delta output sum**).

- Используя дельту выходной суммы определим, насколько скрытый слой влияет на ошибку в выходных данных, умножив ее векторно со второй матрицей весов. Назовем это layer1 ошибкой.

- Вычислить delta output sum для layer1, применив производную нашей сигмоидальной активационной функции.

- Скорректировать весовые коэффициенты с помощью векторного произведения входного слоя со скрытой delta output sum.
Для второго слоя вычислить векторное произведение скрытого слоя и delta output sum.

### Задание

Реализуйте **backward propagation** функцию, которая производит операции, описанные выше.