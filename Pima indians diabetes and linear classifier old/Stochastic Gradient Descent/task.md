Реализуйте метод стохастического градиентного спуска для обучения линейного классификатора.

﻿
Как и в случае с `GradientDescent` метод `fit` должен
возвращать значения $Q$ на каждой итерации. Значение $\eta \in [0, 1]$,
используемое для вычисления оценки $Q$, можно выбрать любое, например,
`1 / len(X)`. Так как величина $Q$ не стабильна, использовать
её для определения сходимости не следует. Вместо этого предлагается
использовать "стратегию оптимиста": сделать ровно `n_iter`
итераций и надеяться, что за это время стохастический градиентный спуск
сойдётся.

Параметр `k` определяет размер случайной подвыборки из
`X`, используемой для вычисления градиента. Такой вариант
стохастического градиентного спуска называется `mini-batch`. 

Обратите внимание, что на данных индейцев пима 1000 итераций алгоритма должна
занимать **не более** 30 секунд.

Воспользуйтесь функциями `test_train_split` и `print_precision_recall` из 
<a href="#Neighbors and Wine#Precision Recall#">урока Neighbors and Wine</a> для оценки качества работы 
реализованных алгоритмов на данных индейцев пима.